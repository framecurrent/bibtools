#! /usr/bin/env python
# -*- mode: python; coding: utf-8 -*-
# Copyright 2014 Peter Williams <peter@newton.cx>
# Licensed under the GNU General Public License, version 3 or higher.

"""
Docstring!
"""

import cookielib, errno, os.path, re, sqlite3, sys, urllib2
import HTMLParser # renamed to html.parser in Python 3.


# Things that are hardcoded but shouldn't be!
#
# The user_agent is because otherwise nature.com gives us the mobile site, which
# happens to not include the easily-recognized <a> tag linking to the paper PDF.
# I don't know exactly what's needed, but if we just send 'Mozilla/5.0' as the UA,
# nature.com gives us a 500 error (!). So I've just copied my current browser's
# UA.

pdfreader = 'evince'
user_agent = 'Mozilla/5.0 (X11; Linux x86_64; rv:27.0) Gecko/20100101 Firefox/27.0'


# Generic app infrastructure

def die (fmt, *args):
    if not len (args):
        raise SystemExit ('error: ' + str (fmt))
    raise SystemExit ('error: ' + fmt % args)


def warn (fmt, *args):
    if not len (args):
        print >>sys.stderr, 'warning:', fmt
    else:
        print >>sys.stderr, 'warning:', fmt % args


class BibError (Exception):
    def __init__ (self, fmt, *args):
        if not len (args):
            self.bibmsg = str (fmt)
        else:
            self.bibmsg = fmt % args

    def __str__ (self):
        return self.bibmsg


class UsageError (BibError):
    pass

class PubLocateError (BibError):
    pass


# The app!

def _make_data_pather ():
    # XXXX
    def pathfunc (*args):
        return os.path.join ('/a/share/bib', *args)
    return pathfunc


datapath = _make_data_pather ()


def _make_user_data_pather ():
    datadir = os.environ.get ('XDG_DATA_HOME',
                              os.path.expanduser ('~/.local/share'))

    def pathfunc (*args):
        return os.path.join (datadir, 'bib', *args)

    return pathfunc


bibpath = _make_user_data_pather ()
dbpath = bibpath ('db.sqlite3')


def mkdir_p (path):
    """That is, create `path` as a directory and all of its parents, ignoring
    errors if they already exist."""

    try:
        os.makedirs (path)
    except OSError as e:
        if e.errno != errno.EEXIST or not os.path.isdir (path):
            raise


def libpath (sha1, ext):
    return bibpath ('lib', sha1[:2], sha1[2:] + '.' + ext)


def ensure_libpath_exists (sha1):
    mkdir_p (bibpath ('lib', sha1[:2]))


def connect ():
    return sqlite3.connect (dbpath)


_arxiv_re_1 = re.compile (r'^\d\d[01]\d\.\d+')
_arxiv_re_2 = re.compile (r'^[a-z-]+/\d+')
_bibcode_re = re.compile (r'^\d\d\d\d[a-zA-Z0-9&]+')
_doi_re = re.compile (r'^10\.\d+/.*')

def locate_pubs (db, text, noneok=False, manyok=False):
    """Given some text that we believe identifies a publication, try to
    retrieve its ID number. A key routine."""

    q = matchtext = None

    if text.startswith ('doi:'):
        q = db.execute ('SELECT id FROM pubs WHERE doi = ?', (text[4:], ))
        matchtext = 'DOI = ' + text[4:]
    elif _doi_re.match (text) is not None:
        q = db.execute ('SELECT id FROM pubs WHERE doi = ?', (text, ))
        matchtext = 'DOI = ' + text
    elif _bibcode_re.match (text) is not None:
        q = db.execute ('SELECT id FROM pubs WHERE bibcode = ?', (text, ))
        matchtext = 'bibcode = ' + text
    elif _arxiv_re_1.match (text) is not None or _arxiv_re_2.match (text) is not None:
        q = db.execute ('SELECT id FROM pubs WHERE arxiv = ?', (text, ))
        matchtext = 'arxiv = ' + text
    else:
        q = db.execute ('SELECT pubid FROM nicknames WHERE nickname = ?', (text, ))
        matchtext = 'nickname = ' + text

    ids = list (t[0] for t in q)

    if not noneok and not len (ids):
        raise PubLocateError ('no publications matched ' + matchtext)
    if not manyok and len (ids) > 1:
        raise PubLocateError ('more than one publication matched ' + matchtext)

    return ids


def locate_pub (db, text, noneok=False):
    ids = locate_pubs (db, text, noneok=noneok, manyok=False)

    if not len (ids):
        return None
    return ids[0]


class NonRedirectingProcessor (urllib2.HTTPErrorProcessor):
    # Copied from StackOverflow q 554446.
    def http_response (self, request, response):
        return response

    https_response = http_response


def get_url_from_redirection (url):
    """Note that we don't go through the proxy class here for convenience, under
    the assumption that all of these redirections involve public information
    that won't require privileged access."""

    opener = urllib2.build_opener (NonRedirectingProcessor ())
    resp = opener.open (url)

    if resp.code not in (301, 302, 303, 307) or 'Location' not in resp.headers:
        die ('expected a redirection response for URL %s but didn\'t get one', url)

    resp.close ()
    return resp.headers['Location']


class HarvardProxyLoginParser (HTMLParser.HTMLParser):
    def __init__ (self):
        HTMLParser.HTMLParser.__init__ (self)
        self.formurl = None
        self.inputs = []


    def handle_starttag (self, tag, attrs):
        if tag == 'form':
            for name, value in attrs:
                if name == 'action':
                    self.formurl = value
                elif name == 'method':
                    if value != 'post':
                        die ('unexpected form method')
        elif tag == 'input':
            iname = ivalue = None

            for name, value in attrs:
                if name == 'name':
                    iname = value
                elif name == 'value':
                    ivalue = value

            if iname is None or ivalue is None:
                die ('missing form input information')

            self.inputs.append ((iname, ivalue))


def parse_http_html (resp, parser):
    import codecs
    debug = False

    charset = resp.headers.getparam ('charset')
    if charset is None:
        charset = 'ISO-8859-1'

    dec = codecs.getincrementaldecoder (charset) ()

    if debug:
        f = open ('debug.html', 'w')

    while True:
        d = resp.read (4096)
        if not len (d):
            text = dec.decode ('', final=True)
            parser.feed (text)
            break

        if debug:
            f.write (d)

        text = dec.decode (d)
        parser.feed (text)

    if debug:
        f.close ()

    resp.close ()
    parser.close ()
    return parser


class HarvardProxy (object):
    suffix = '.ezp-prod1.hul.harvard.edu'
    loginurl = 'https://www.pin1.harvard.edu/cas/login'
    forwardurl = 'http://ezp-prod1.hul.harvard.edu/connect'

    inputs = [
        ('compositeAuthenticationSourceType', 'PIN'),
        ('username', '70576465'),
        ('password', None),
    ]

    def __init__ (self):
        self.cj = cookielib.CookieJar ()
        self.opener = urllib2.build_opener (urllib2.HTTPRedirectHandler (),
                                            urllib2.HTTPCookieProcessor (self.cj))

        # XXX This doesn't quite belong here.
        self.opener.addheaders = [('User-Agent', user_agent)]

        self.password = open ('scarytemp').readline ().strip () # XXXXXXX


    def login (self, resp):
        # XXX we should verify the SSL cert of the counterparty, lest we send
        # our password to malicious people.
        parser = parse_http_html (resp, HarvardProxyLoginParser ())

        if parser.formurl is None:
            die ('malformed proxy page response?')

        from urlparse import urljoin
        posturl = urljoin (resp.url, parser.formurl)
        values = {}

        for name, value in parser.inputs:
            values[name] = value

        for name, value in self.inputs:
            if value is None:
                value = self.password
            values[name] = value

        from urllib import urlencode # yay terrible Python APIs
        req = urllib2.Request (posturl, urlencode (values))
        # The response will redirect to the original target page.
        return self.opener.open (req)


    def open (self, url):
        from urlparse import urlparse, urlunparse
        scheme, loc, path, params, query, frag = urlparse (url)
        loc += self.suffix
        proxyurl = urlunparse ((scheme, loc, path, params, query, frag))

        resp = self.opener.open (proxyurl)

        if resp.url.startswith (self.loginurl):
            resp = self.login (resp)

        if resp.url.startswith (self.forwardurl):
            # Sometimes we get forwarded to a separate cookie-setting page
            # that requires us to re-request the original URL.
            resp = self.opener.open (proxyurl)

        return resp


    def unmangle (self, url):
        if url is None:
            return None # convenience

        from urlparse import urlparse, urlunparse

        scheme, loc, path, params, query, frag = urlparse (url)
        if not loc.endswith (self.suffix):
            return url

        loc = loc[:-len (self.suffix)]
        return urlunparse ((scheme, loc, path, params, query, frag))


def setup_proxy ():
    return HarvardProxy ()


class PDFUrlScraper (HTMLParser.HTMLParser):
    """Observed places to look for PDF URLs:

    <meta> tag with name=citation_pdf_url -- IOP
    <a> tag with id=download-pdf -- Nature (non-mobile site, newer)
    <a> tag with class=download-pdf -- Nature (older)
    """

    def __init__ (self):
        HTMLParser.HTMLParser.__init__ (self)
        self.pdfurl = None


    def handle_starttag (self, tag, attrs):
        if self.pdfurl is not None:
            return

        if tag == 'meta':
            mname = content = None

            for aname, avalue in attrs:
                if aname == 'name':
                    mname = avalue
                elif aname == 'content':
                    content = avalue

            if mname == 'citation_pdf_url':
                self.pdfurl = content
        elif tag == 'a':
            attrs = dict (attrs)
            if attrs.get ('id') == 'download-pdf':
                self.pdfurl = attrs['href']
            elif attrs.get ('class') == 'download-pdf':
                self.pdfurl = attrs['href']


def scrape_pdf_url (resp):
    parser = parse_http_html (resp, PDFUrlScraper ())
    if parser.pdfurl is None:
        return None

    from urlparse import urljoin
    return urljoin (resp.url, parser.pdfurl)


def doi_to_journal_url (doi):
    return get_url_from_redirection ('http://dx.doi.org/' + urllib2.quote (doi))


def bibcode_to_maybe_pdf_url (bibcode):
    """If ADS doesn't have a fulltext link for a given bibcode, it will return a link
    to articles.ads.harvard.edu that in turn yields an HTML error page.

    Also, the Location header returned by the ADS server appears to be slightly broken,
    with the &'s in the URL being HTML entity-encoded to &amp;s."""

    url = ('http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ARTICLE&bibcode='
           + urllib2.quote (bibcode))
    pdfurl = get_url_from_redirection (url)
    return pdfurl.replace ('&amp;', '&')


def get_pdf (db, proxy, id):
    r = list (db.execute ('SELECT arxiv, bibcode, doi FROM pubs '
                          'WHERE id = ?', (id, )))
    assert len (r) == 1, 'unexpected ID thingie %d' % id
    arxiv, bibcode, doi = r[0]
    pdfurl = None

    if doi is not None:
        jurl = doi_to_journal_url (doi)
        print 'Attempting to scrape', jurl, '...'
        pdfurl = proxy.unmangle (scrape_pdf_url (proxy.open (jurl)))

    if pdfurl is None and bibcode is not None:
        # This never returns None: ADS will always give a URL, but it may just
        # be that the URL resolves to a 404 page saying that ADS has no PDF
        # available. Thus, this technique is always our last resort.
        pdfurl = bibcode_to_maybe_pdf_url (bibcode)

    if pdfurl is None:
        return None

    # OK, we can now download and register the PDF. TODO: progress reporting,
    # etc.

    import hashlib
    mkdir_p (bibpath ('lib'))
    temppath = bibpath ('lib', 'incoming.pdf')
    s = hashlib.sha1 ()

    print 'Trying', pdfurl, '...'

    try:
        resp = proxy.open (pdfurl)
    except urllib2.HTTPError as e:
        from urlparse import urlparse
        if e.code == 404 and urlparse (pdfurl)[1] == 'articles.adsabs.harvard.edu':
            warn ('ADS doesn\'t actually have the PDF on file')
            return None # ADS gave us a URL that turned out to be a lie.
        raise

    first = True

    with open (temppath, 'w') as f:
        while True:
            b = resp.read (4096)

            if first:
                if len (b) < 4 or b[:4] != '%PDF':
                    warn ('response does not seem to be a PDF')
                    resp.close ()
                    f.close ()
                    os.unlink (temppath)
                    return None
                first = False

            if not len (b):
                break

            s.update (b)
            f.write (b)

    sha1 = s.hexdigest ()
    ensure_libpath_exists (sha1)
    destpath = libpath (sha1, 'pdf')
    os.rename (temppath, destpath)

    db.execute ('INSERT OR REPLACE INTO pdfs VALUES (?, ?)', (sha1, id))

    return sha1


# Commands

def cmd_init (argv):
    mkdir_p (bibpath ())

    if os.path.exists (dbpath):
        die ('the file "%s" already exists', dbpath)

    with connect () as db:
        try:
            init = open (datapath ('schema.sql')).read ()
            db.executescript (init)
        except sqlite3.OperationalError as e:
            die ('cannot initialize "%s": %s', dbpath, e)


def cmd_info (argv):
    if len (argv) != 2:
        raise UsageError ('expected exactly 1 argument')

    idtext = argv[1]

    with connect () as db:
        # TODO: should be OK to match multiple publications and print them all
        # out.

        try:
            id = locate_pub (db, idtext)
        except PubLocateError as e:
            die (e)

        for row in db.execute ('SELECT * FROM pubs WHERE id = ?', (id, )):
            _, abstract, arxiv, bibcode, doi, firstsurname, title, year = row

            if firstsurname is None:
                firstsurname = '(no author)'
            if year is None:
                year = 'no year'
            if title is None:
                title = '(no title)'

            print '%s*(%s): %s' % (firstsurname, year, title)

            if arxiv is not None:
                print 'arxiv:', arxiv
            if bibcode is not None:
                print 'bibcode:', bibcode
            if doi is not None:
                print 'DOI:', doi

            if abstract is not None:
                print
                # XXX linewrapping etc
                print abstract


_bibtex_replacements = (
    '\\&ap;', u'~',
    '\\&#177;', u'±',
    '\&gt;~', u'⪞',
    '\&lt;~', u'⪝',
    '{', u'',
    '}', u'',
    '<SUP>', u'^',
    '</SUP>', u'',
    '<SUB>', u'_',
    '</SUB>', u'',
    'Delta', u'Δ',
    'Omega', u'Ω',
    '( ', u'(',
    ' )', u')',
    '[ ', u'[',
    ' ]', u']',
    ' ,', u',',
    ' .', u'.',
    ' ;', u';',
    '\t', u' ',
    '  ', u' ',
)

def _fix_bibtex (text):
    """Ugggghhh. So many problems."""

    if text is None:
        return None

    text = unicode (text)

    for i in xrange (0, len (_bibtex_replacements), 2):
        text = text.replace (_bibtex_replacements[i], _bibtex_replacements[i+1])
    return text


def sniff_url (url):
    p = 'http://dx.doi.org/'
    if url.startswith (p):
        return 'doi', urllib2.unquote (url[len (p):])

    p = 'http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode='
    if url.startswith (p):
        return 'bibcode', urllib2.unquote (url[len (p):])

    p = 'http://labs.adsabs.harvard.edu/ui/abs/'
    if url.startswith (p):
        return 'bibcode', urllib2.unquote (url[len (p):])

    p = 'http://arxiv.org/abs/'
    if url.startswith (p):
        return 'arxiv', urllib2.unquote (url[len (p):])

    p = 'http://arxiv.org/pdf/'
    if url.startswith (p):
        return 'arxiv', urllib2.unquote (url[len (p):])

    return None, None


def _ingest_one (db, rec):
    abstract = rec.get ('abstract')
    arxiv = rec.get ('eprint')
    bibcode = None
    doi = rec.get ('doi')
    nickname = rec.get ('id')
    title = rec.get ('title')
    year = rec.get ('year')

    if year is not None:
        year = int (year)

    if 'author' in rec:
        t = _fix_bibtex (rec['author'][0])
        firstsurname = t.split (',', 1)[0]
    else:
        firstsurname = None

    abstract = _fix_bibtex (abstract)
    title = _fix_bibtex (title)

    # Augment information with what we can get from URLs

    urlinfo = []

    if 'url' in rec:
        urlinfo.append (sniff_url (rec['url']))

    for k, v in rec.iteritems ():
        if k.startswith ('citeulike-linkout-'):
            urlinfo.append (sniff_url (v))

    for kind, info in urlinfo:
        if kind is None:
            continue

        if kind == 'bibcode' and bibcode is None:
            bibcode = info

        if kind == 'doi' and doi is None:
            doi = info

        if kind == 'arxiv' and arxiv is None:
            arxiv = info

    # Ready to insert.

    c = db.cursor ()
    c.execute ('INSERT INTO pubs VALUES (NULL, ?, ?, ?, ?, ?, ?, ?)',
               (abstract, arxiv, bibcode, doi, firstsurname, title, year))

    if nickname is not None:
        try:
            c.execute ('INSERT INTO nicknames VALUES (?, ?)',
                       (nickname, c.lastrowid))
        except sqlite3.IntegrityError:
            die ('duplicated pub nickname "%s"', nickname)


def cmd_ingest (argv):
    if len (argv) != 2:
        raise UsageError ('expected exactly 1 argument')

    bibpath = argv[1]
    from bibtexparser.bparser import BibTexParser
    from bibtexparser.customization import author, type, convert_to_unicode

    custom = lambda r: author (type (convert_to_unicode (r)))

    with open (bibpath) as bibfile, connect () as db:
        bp = BibTexParser (bibfile, customization=custom)

        for rec in bp.get_entry_list ():
            _ingest_one (db, rec)


def cmd_jpage (argv):
    if len (argv) != 2:
        raise UsageError ('expected exactly 1 argument')

    import webbrowser
    from urllib2 import quote

    idtext = argv[1]

    with connect () as db:
        try:
            id = locate_pub (db, idtext)
        except PubLocateError as e:
            die (e)

        doi = list (db.execute ('SELECT doi FROM pubs WHERE id = ?', (id, )))[0][0]
        if doi is None:
            die ('cannot open journal website: no DOI for record')

        url = 'http://dx.doi.org/' + quote (doi)
        webbrowser.open (url)


def cmd_read (argv):
    if len (argv) != 2:
        raise UsageError ('expected exactly 1 argument')

    idtext = argv[1]

    with connect () as db:
        try:
            id = locate_pub (db, idtext)
        except PubLocateError as e:
            die (e)

        res = list (db.execute ('SELECT sha1 FROM pdfs WHERE pubid = ?', (id, )))
        if len (res):
            sha1 = res[0][0]
        else:
            proxy = setup_proxy ()
            sha1 = get_pdf (db, proxy, id)

    if sha1 is None:
        die ('no saved PDF for %s, and cannot figure out how to download it', idtext)

    # XXX may want to centralize this; also, close FD's, etc.

    try:
        pid = os.fork ()
    except OSError as e:
        die ('cannot fork() first time: %s', e)

    if pid == 0:
        os.setsid () # become new session leader, apparently a good thing to do.

        try:
            pid2 = os.fork ()
        except OSError as e:
            die ('cannot fork() second time: %s', e)

        if pid == 0:
            os.execlp (pdfreader, pdfreader, libpath (sha1, 'pdf'))
        else:
            os._exit (0)
    else:
        pass # parent has nothing to do


def cmd_setpdf (argv):
    if len (argv) != 3:
        raise UsageError ('expected exactly 2 arguments')

    idtext = argv[1]
    pdfpath = argv[2]

    import hashlib, shutil

    with connect () as db:
        # Check that we know what pub we're talking about
        try:
            id = locate_pub (db, idtext)
        except PubLocateError as e:
            die (e)

        # Get SHA1 of the PDF
        with open (pdfpath) as f:
            s = hashlib.sha1 ()

            while True:
                b = f.read (4096)
                if not len (b):
                    break

                s.update (b)

            sha1 = s.hexdigest ()

        # Copy it into the library
        ensure_libpath_exists (sha1)
        dest = libpath (sha1, 'pdf')
        shutil.copyfile (pdfpath, dest)

        # Update the DB
        db.execute ('INSERT OR REPLACE INTO pdfs VALUES (?, ?)', (sha1, id))


# Toplevel driver infrastructure

def usage ():
    print 'usage goes here'


def driver (argv):
    if len (argv) == 1 or argv[1] == '--help':
        usage ()
        return

    cmdname = argv[1]
    cmdfunc = globals ().get ('cmd_' + cmdname.replace ('-', '_'))

    if not callable (cmdfunc):
        die ('"%s" is not a recognized subcommand; run me without '
             'arguments for usage help', cmdname)

    try:
        cmdfunc (argv[1:])
    except UsageError as ue:
        # TODO: synopsize command-specific usage help as an attribute on the
        # function (so we can auto-gen a multi-command usage summary too)
        raise SystemExit ('usage error: ' + ue.bibmsg)

if __name__ == '__main__':
    driver (sys.argv)
