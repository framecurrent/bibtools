#! /usr/bin/env python
# -*- mode: python; coding: utf-8 -*-
# Copyright 2014 Peter Williams <peter@newton.cx>
# Licensed under the GNU General Public License, version 3 or higher.

"""
Docstring!
"""

import collections, cookielib, errno, os.path, re, sqlite3, sys, urllib2
import HTMLParser # renamed to html.parser in Python 3.


# Things that are hardcoded but shouldn't be!
#
# The user_agent is because otherwise nature.com gives us the mobile site, which
# happens to not include the easily-recognized <a> tag linking to the paper PDF.
# I don't know exactly what's needed, but if we just send 'Mozilla/5.0' as the UA,
# nature.com gives us a 500 error (!). So I've just copied my current browser's
# UA.

pdfreader = 'evince'
user_agent = 'Mozilla/5.0 (X11; Linux x86_64; rv:27.0) Gecko/20100101 Firefox/27.0'
openssl_cmd = '/usr/bin/openssl'
crossref_api_key = 'peter-bulk@newton.cx'


# Generic app infrastructure

def die (fmt, *args):
    if not len (args):
        raise SystemExit ('error: ' + str (fmt))
    raise SystemExit ('error: ' + fmt % args)


def warn (fmt, *args):
    if not len (args):
        print >>sys.stderr, 'warning:', fmt
    else:
        print >>sys.stderr, 'warning:', fmt % args


class BibError (Exception):
    def __init__ (self, fmt, *args):
        if not len (args):
            self.bibmsg = str (fmt)
        else:
            self.bibmsg = fmt % args

    def __str__ (self):
        return self.bibmsg


class UsageError (BibError):
    pass

class PubLocateError (BibError):
    pass


_whitespace_re = re.compile (r'\s+')

def squish_spaces (text):
    return _whitespace_re.sub (' ', text).strip ()


# The app!

def _make_data_pather ():
    # XXXX
    def pathfunc (*args):
        return os.path.join ('/a/share/bib', *args)
    return pathfunc


datapath = _make_data_pather ()


def _make_user_data_pather ():
    datadir = os.environ.get ('XDG_DATA_HOME',
                              os.path.expanduser ('~/.local/share'))

    def pathfunc (*args):
        return os.path.join (datadir, 'bib', *args)

    return pathfunc


bibpath = _make_user_data_pather ()
dbpath = bibpath ('db.sqlite3')


def mkdir_p (path):
    """That is, create `path` as a directory and all of its parents, ignoring
    errors if they already exist."""

    try:
        os.makedirs (path)
    except OSError as e:
        if e.errno != errno.EEXIST or not os.path.isdir (path):
            raise


def libpath (sha1, ext):
    return bibpath ('lib', sha1[:2], sha1[2:] + '.' + ext)


def ensure_libpath_exists (sha1):
    mkdir_p (bibpath ('lib', sha1[:2]))


# The database!

def connect ():
    return sqlite3.connect (dbpath, factory=BibDB)


PubRow = collections.namedtuple ('PubRow',
                                 'id abstract arxiv bibcode doi firstsurname '
                                 'keep title year'.split ())

AuthorNameRow = collections.namedtuple ('AuthorNameRow',
                                        ['name'])

AuthorRow = collections.namedtuple ('AuthorRow',
                                    'pubid idx authid'.split ())

NicknameRow = collections.namedtuple ('NicknameRow',
                                      'nickname pubid'.split ())

PdfRow = collections.namedtuple ('PdfRow',
                                 'sha1 pubid'.split ())


def nt_augment (ntclass, **vals):
    for k in vals.iterkeys ():
        if k not in ntclass._fields:
            raise ValueError ('illegal field "%s" for creating %s instance'
                              % (k, ntclass.__name__))
    return ntclass (*tuple (vals.get (k) for k in ntclass._fields))


def parse_author (text):
    first, last = text.rsplit (' ', 1)
    return first, last.replace ('_', ' ')


class BibDB (sqlite3.Connection):
    def getfirst (self, fmt, *args):
        return self.execute (fmt, args).fetchone ()


    def locate_pubs (self, text, noneok=False, manyok=False):
        kind, text = classify_pub_ref (text)

        q = matchtext = None

        if kind == 'doi':
            q = self.execute ('SELECT id FROM pubs WHERE doi = ?', (text, ))
            matchtext = 'DOI = ' + text
        elif kind == 'bibcode':
            q = self.execute ('SELECT id FROM pubs WHERE bibcode = ?', (text, ))
            matchtext = 'bibcode = ' + text
        elif kind == 'arxiv':
            q = self.execute ('SELECT id FROM pubs WHERE arxiv = ?', (text, ))
            matchtext = 'arxiv = ' + text
        elif kind == 'nickname':
            q = self.execute ('SELECT pubid FROM nicknames WHERE nickname = ?', (text, ))
            matchtext = 'nickname = ' + text
        else:
            assert False

        ids = list (t[0] for t in q)

        if not noneok and not len (ids):
            raise PubLocateError ('no publications matched ' + matchtext)
        if not manyok and len (ids) > 1:
            raise PubLocateError ('more than one publication matched ' + matchtext)

        return ids


    def locate_pub (self, text, noneok=False, autolearn=False):
        if autolearn:
            noneok = True

        ids = self.locate_pubs (text, noneok=noneok, manyok=False)

        if not len (ids):
            if autolearn:
                return self.learn_pub (autolearn_pub (text))
            return None

        return ids[0]


    def pub_query (self, fmt, *args):
        c = self.cursor ()
        c.row_factory = lambda curs, tup: PubRow (*tup)
        return c.execute ('SELECT * FROM pubs WHERE ' + fmt, args)


    def try_get_pdf_for_id (self, proxy, id):
        r = self.getfirst ('SELECT arxiv, bibcode, doi FROM pubs WHERE id = ?', id)
        arxiv, bibcode, doi = r

        mkdir_p (bibpath ('lib'))
        temppath = bibpath ('lib', 'incoming.pdf')

        sha1 = try_fetch_pdf (proxy, temppath,
                              arxiv=arxiv, bibcode=bibcode, doi=doi)
        if sha1 is None:
            return None

        ensure_libpath_exists (sha1)
        destpath = libpath (sha1, 'pdf')
        os.rename (temppath, destpath)
        self.execute ('INSERT OR REPLACE INTO pdfs VALUES (?, ?)', (sha1, id))
        return sha1


    def learn_pub_authors (self, pubid, authors):
        c = self.cursor ()

        for idx, auth in enumerate (authors):
            # Based on reading StackExchange, there's no cleaner way to do this,
            # but the SELECT should be snappy.
            c.execute ('INSERT OR IGNORE INTO author_names VALUES (?)',
                       (auth, ))
            row = self.getfirst ('SELECT oid FROM author_names WHERE name = ?', auth)[0]
            c.execute ('INSERT OR REPLACE INTO authors VALUES (?, ?, ?)',
                       (pubid, idx, row))


    def get_pub_authors (self, pubid):
        return (parse_author (a[0]) for a in
                self.execute ('SELECT name FROM authors AS au, author_names AS an '
                              'WHERE au.authid == an.oid AND au.pubid == ? '
                              'ORDER BY idx', (pubid, )))


    def learn_pub (self, info):
        """Note that `info` will be mutated."""

        authors = info.pop ('authors', ())
        nickname = info.pop ('nickname', None)

        if 'abstract' in info:
            info['abstract'] = squish_spaces (info['abstract'])
        if 'title' in info:
            info['title'] = squish_spaces (info['title'])

        row = nt_augment (PubRow, **info)
        c = self.cursor ()
        c.execute ('INSERT INTO pubs VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)', row)
        pubid = c.lastrowid

        if authors is not None:
            self.learn_pub_authors (pubid, authors)

        if nickname is not None:
            try:
                c.execute ('INSERT INTO nicknames VALUES (?, ?)',
                           (nickname, pubid))
            except sqlite3.IntegrityError:
                die ('duplicated pub nickname "%s"', nickname)

        return pubid


# Bibliography logic

_arxiv_re_1 = re.compile (r'^\d\d[01]\d\.\d+')
_arxiv_re_2 = re.compile (r'^[a-z-]+/\d+')
_bibcode_re = re.compile (r'^\d\d\d\d[a-zA-Z0-9&]+')
_doi_re = re.compile (r'^10\.\d+/.*')


def classify_pub_ref (text):
    """Given some text that we believe identifies a publication, try to
    figure out how it does so."""

    if text.startswith ('doi:'):
        return 'doi', text[4:]

    if _doi_re.match (text) is not None:
        return 'doi', text

    if _bibcode_re.match (text) is not None:
        return 'bibcode', text

    if _arxiv_re_1.match (text) is not None:
        return 'arxiv', text

    if _arxiv_re_2.match (text) is not None:
        return 'arxiv', text

    if text.startswith ('arxiv:'):
        return 'arxiv', text[6:]

    return 'nickname', text


def try_fetch_pdf (proxy, destpath, arxiv=None, bibcode=None, doi=None):
    """Given reference information, download a PDF to a specified path. Returns
    the SHA1 sum of the PDF as a hexadecimal string, or None if we couldn't
    figure out how to download it."""

    pdfurl = None

    if doi is not None:
        jurl = doi_to_journal_url (doi)
        print 'Attempting to scrape', jurl, '...'
        pdfurl = proxy.unmangle (scrape_pdf_url (proxy.open (jurl)))

    if pdfurl is None and bibcode is not None:
        # This never returns None: ADS will always give a URL, but it may just
        # be that the URL resolves to a 404 page saying that ADS has no PDF
        # available. Thus, this technique is always our last resort.
        pdfurl = bibcode_to_maybe_pdf_url (bibcode)

    if pdfurl is None and arxiv is not None:
        # Always prefer non-preprints. I need to straighten out how I'm going
        # to deal with them ...
        pdfurl = 'http://arxiv.org/pdf/' + urllib2.quote (arxiv) + '.pdf'

    if pdfurl is None:
        return None

    # OK, we can now download and register the PDF. TODO: progress reporting,
    # etc.

    import hashlib
    s = hashlib.sha1 ()

    print 'Trying', pdfurl, '...'

    try:
        resp = proxy.open (pdfurl)
    except urllib2.HTTPError as e:
        from urlparse import urlparse
        if e.code == 404 and urlparse (pdfurl)[1] == 'articles.adsabs.harvard.edu':
            warn ('ADS doesn\'t actually have the PDF on file')
            return None # ADS gave us a URL that turned out to be a lie.
        raise

    first = True

    with open (destpath, 'w') as f:
        while True:
            b = resp.read (4096)

            if first:
                if len (b) < 4 or b[:4] != '%PDF':
                    warn ('response does not seem to be a PDF')
                    resp.close ()
                    f.close ()
                    os.unlink (temppath)
                    return None
                first = False

            if not len (b):
                break

            s.update (b)
            f.write (b)

    return s.hexdigest ()


# Handling of the user's login secret. I've made the decision to store this on
# disk without requiring user input to access the secret -- i.e., this is what
# Firefox does for user website passwords when a Master Password hasn't been
# enabled. I feel sketchy about this, but if Mozilla is OK with it, then so
# am I.
#
# I follow what I believe to be Firefox's storage strategy, which is to use
# symmetric encryption to store the secret on disk, with the relevant
# encryption keys also stored on disk. Obviously this only provides security
# against a completely unmotivated attacker, but it prevents accidental
# disclosure, and again, this approach seems to be good enough for Mozilla.
#
# There are crypto modules for Python, but the examples I saw were lengthy and
# the modules aren't preinstalled on my computer (therefore most people
# probably don't have them), so I've farmed out the work to the openssl CLI.
#
# Because we're in Python, I'm sure that we're doing all sorts of unfortunate
# things like keeping the decrypted secret in memory for too long, etc.


def load_secret_keys ():
    key = iv = None

    with open (bibpath ('secret.key')) as kfile:
        for line in kfile:
            line = line.strip ()

            if line.startswith ('key='):
                key = line[4:]
            elif line.startswith ('iv ='):
                iv = line[4:]

    if key is None or iv is None:
        die ('damaged secret key file %s?', bibpath ('secret.key'))

    return key, iv


def set_terminal_echo (enabled):
    import termios

    fd = sys.stdin.fileno ()
    ifl, ofl, cfl, lfl, isp, osp, cc = termios.tcgetattr (fd)

    if enabled:
        lfl |= termios.ECHO
    else:
        lfl &= ~termios.ECHO

    termios.tcsetattr (fd, termios.TCSANOW,
                       [ifl, ofl, cfl, lfl, isp, osp, cc])


def store_user_secret ():
    import random, string, subprocess

    # Generate a random password for the key generation. Python SystemRandom
    # uses /dev/urandom, so it's possible that the password may be derived
    # in a low-entropy state, but ... meh.

    sys = random.SystemRandom ()
    pool = string.digits + string.letters + string.punctuation
    keypass = ''.join (sys.choice (pool) for _ in xrange (64))

    # Generate the static keys

    os.umask (0o177)

    kfd = os.open (bibpath ('secret.key'),
                   os.O_WRONLY | os.O_CREAT | os.O_TRUNC,
                   0o600) # just in case ...

    with os.fdopen (kfd, 'w') as kfile:
        subprocess.check_call ([openssl_cmd, 'enc', '-aes-256-cbc', '-k', keypass,
                                '-P', '-md', 'sha1'], stdout=kfile, shell=False,
                               close_fds=True)

    # Encrypt and store password

    sfd = os.open (bibpath ('secret.bin'),
                   os.O_WRONLY | os.O_CREAT | os.O_TRUNC,
                   0o600) # just in case ...

    key, iv = load_secret_keys ()

    try:
        set_terminal_echo (False)
        print 'Enter password, then Enter, then control-D twice:'

        with os.fdopen (sfd, 'w') as sfile:
            subprocess.check_call ([openssl_cmd, 'enc', '-aes-256-cbc', '-e', '-K',
                                    key, '-iv', iv], stdout=sfile, shell=False,
                                   close_fds=True)

        print 'Success.'
    finally:
        set_terminal_echo (True)


def load_user_secret ():
    import subprocess

    key, iv = load_secret_keys ()
    secret = subprocess.check_output ([openssl_cmd, 'enc', '-aes-256-cbc', '-d',
                                       '-K', key, '-iv', iv, '-in',
                                       bibpath ('secret.bin')], shell=False,
                                      close_fds=True)
    secret = secret[:-1] # strip trailing newline imposed by our input method
    return secret


# Web scraping, proxy, etc. helpers.

class NonRedirectingProcessor (urllib2.HTTPErrorProcessor):
    # Copied from StackOverflow q 554446.
    def http_response (self, request, response):
        return response

    https_response = http_response


def get_url_from_redirection (url):
    """Note that we don't go through the proxy class here for convenience, under
    the assumption that all of these redirections involve public information
    that won't require privileged access."""

    opener = urllib2.build_opener (NonRedirectingProcessor ())
    resp = opener.open (url)

    if resp.code not in (301, 302, 303, 307) or 'Location' not in resp.headers:
        die ('expected a redirection response for URL %s but didn\'t get one', url)

    resp.close ()
    return resp.headers['Location']


class HarvardProxyLoginParser (HTMLParser.HTMLParser):
    def __init__ (self):
        HTMLParser.HTMLParser.__init__ (self)
        self.formurl = None
        self.inputs = []


    def handle_starttag (self, tag, attrs):
        if tag == 'form':
            attrs = dict (attrs)
            self.formurl = attrs.get ('action')
            if attrs.get ('method') != 'post':
                die ('unexpected form method')
        elif tag == 'input':
            attrs = dict (attrs)
            if 'name' not in attrs or 'value' not in attrs:
                die ('missing form input information')
            self.inputs.append ((attrs['name'], attrs['value']))


def parse_http_html (resp, parser):
    import codecs
    debug = False

    charset = resp.headers.getparam ('charset')
    if charset is None:
        charset = 'ISO-8859-1'

    dec = codecs.getincrementaldecoder (charset) ()

    if debug:
        f = open ('debug.html', 'w')

    while True:
        d = resp.read (4096)
        if not len (d):
            text = dec.decode ('', final=True)
            parser.feed (text)
            break

        if debug:
            f.write (d)

        text = dec.decode (d)
        parser.feed (text)

    if debug:
        f.close ()

    resp.close ()
    parser.close ()
    return parser


class HarvardProxy (object):
    suffix = '.ezp-prod1.hul.harvard.edu'
    loginurl = 'https://www.pin1.harvard.edu/cas/login'
    forwardurl = 'http://ezp-prod1.hul.harvard.edu/connect'

    inputs = [
        ('compositeAuthenticationSourceType', 'PIN'),
        ('username', '70576465'),
        ('password', None),
    ]

    def __init__ (self):
        self.cj = cookielib.CookieJar ()
        self.opener = urllib2.build_opener (urllib2.HTTPRedirectHandler (),
                                            urllib2.HTTPCookieProcessor (self.cj))

        # XXX This doesn't quite belong here.
        self.opener.addheaders = [('User-Agent', user_agent)]

        # It's not good to have this hanging around in memory, but Python
        # strings are immutable and we have no idea what (if anything) `del
        # self.password` would accomplish, so I don't think we can really do
        # better.
        self.password = load_user_secret ()


    def login (self, resp):
        # XXX we should verify the SSL cert of the counterparty, lest we send
        # our password to malicious people.
        parser = parse_http_html (resp, HarvardProxyLoginParser ())

        if parser.formurl is None:
            die ('malformed proxy page response?')

        from urlparse import urljoin
        posturl = urljoin (resp.url, parser.formurl)
        values = {}

        for name, value in parser.inputs:
            values[name] = value

        for name, value in self.inputs:
            if value is None:
                value = self.password
            values[name] = value

        from urllib import urlencode # yay terrible Python APIs
        req = urllib2.Request (posturl, urlencode (values))
        # The response will redirect to the original target page.
        return self.opener.open (req)


    def open (self, url):
        from urlparse import urlparse, urlunparse
        scheme, loc, path, params, query, frag = urlparse (url)

        if loc.endswith ('arxiv.org'):
            # For whatever reason, the proxy server doesn't work
            # if we try to access Arxiv with it.
            proxyurl = url
        else:
            loc += self.suffix
            proxyurl = urlunparse ((scheme, loc, path, params, query, frag))

        resp = self.opener.open (proxyurl)

        if resp.url.startswith (self.loginurl):
            resp = self.login (resp)

        if resp.url.startswith (self.forwardurl):
            # Sometimes we get forwarded to a separate cookie-setting page
            # that requires us to re-request the original URL.
            resp = self.opener.open (proxyurl)

        return resp


    def unmangle (self, url):
        if url is None:
            return None # convenience

        from urlparse import urlparse, urlunparse

        scheme, loc, path, params, query, frag = urlparse (url)
        if not loc.endswith (self.suffix):
            return url

        loc = loc[:-len (self.suffix)]
        return urlunparse ((scheme, loc, path, params, query, frag))


def setup_proxy ():
    return HarvardProxy ()


class PDFUrlScraper (HTMLParser.HTMLParser):
    """Observed places to look for PDF URLs:

    <meta> tag with name=citation_pdf_url -- IOP
    <a> tag with id=download-pdf -- Nature (non-mobile site, newer)
    <a> tag with class=download-pdf -- Nature (older)
    """

    def __init__ (self):
        HTMLParser.HTMLParser.__init__ (self)
        self.pdfurl = None


    def handle_starttag (self, tag, attrs):
        if self.pdfurl is not None:
            return

        if tag == 'meta':
            attrs = dict (attrs)
            if attrs.get ('name') == 'citation_pdf_url':
                self.pdfurl = attrs['content']
        elif tag == 'a':
            attrs = dict (attrs)
            if attrs.get ('id') == 'download-pdf':
                self.pdfurl = attrs['href']
            elif attrs.get ('class') == 'download-pdf':
                self.pdfurl = attrs['href']


def scrape_pdf_url (resp):
    parser = parse_http_html (resp, PDFUrlScraper ())
    if parser.pdfurl is None:
        return None

    from urlparse import urljoin
    return urljoin (resp.url, parser.pdfurl)


def doi_to_journal_url (doi):
    return get_url_from_redirection ('http://dx.doi.org/' + urllib2.quote (doi))


def bibcode_to_maybe_pdf_url (bibcode):
    """If ADS doesn't have a fulltext link for a given bibcode, it will return a link
    to articles.ads.harvard.edu that in turn yields an HTML error page.

    Also, the Location header returned by the ADS server appears to be slightly broken,
    with the &'s in the URL being HTML entity-encoded to &amp;s."""

    url = ('http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ARTICLE&bibcode='
           + urllib2.quote (bibcode))
    pdfurl = get_url_from_redirection (url)
    return pdfurl.replace ('&amp;', '&')


def doi_to_maybe_bibcode (doi):
    bibcode = None

    url = ('http://adsabs.harvard.edu/cgi-bin/nph-abs_connect?'
           'data_type=Custom&format=%25R&nocookieset=1&doi=' +
           urllib2.quote (doi))
    lastnonempty = None

    for line in urllib2.urlopen (url):
        line = line.strip ()
        if len (line):
            lastnonempty = line

    if lastnonempty is None:
        return None
    if lastnonempty.startswith ('Retrieved 0 abstracts'):
        return None

    return lastnonempty


# Autolearning publications

def autolearn_pub (text):
    kind, text = classify_pub_ref (text)

    if kind == 'doi':
        # ADS seems to have better data quality.
        bc = doi_to_maybe_bibcode (text)
        if bc is not None:
            print 'Associated', doi, 'to', bc
            kind, text = 'bibcode', bc

    if kind == 'doi':
        return autolearn_doi (text)

    if kind == 'bibcode':
        return autolearn_bibcode (text)

    if kind == 'arxiv':
        return autolearn_arxiv (text)

    die ('cannot auto-learn publication "%s"', text)


def _translate_ads_name (name):
    pieces = [x.strip () for x in name.split (',', 1)]
    surname = pieces[0].replace (' ', '_')

    if len (pieces) > 1:
        return pieces[1] + ' ' + surname
    return surname


def _autolearn_bibcode_tag (info, tag, text):
    # TODO: all authors, etc.

    if tag == 'T':
        info['title'] = text
    elif tag == 'D':
        info['year'] = int (text.split ('/')[-1])
    elif tag == 'B':
        info['abstract'] = text
    elif tag == 'A':
        info['firstsurname'] = text.split (',', 1)[0]
        info['authors'] = [_translate_ads_name (n) for n in text.split (';')]
    elif tag == 'Y':
        subdata = dict (s.strip ().split (': ', 1)
                        for s in text.split (';'))

        if 'DOI' in subdata:
            info['doi'] = subdata['DOI']
        if 'eprintid' in subdata:
            value = subdata['eprintid']
            if value.startswith ('arXiv:'):
                info['arxiv'] = value[6:]


def autolearn_bibcode (bibcode):
    url = ('http://adsabs.harvard.edu/cgi-bin/nph-abs_connect?'
           'data_type=PORTABLE&nocookieset=1&bibcode=' + urllib2.quote (bibcode))

    info = {'bibcode': bibcode, 'keep': 0} # because we're autolearning
    curtag = curtext = None

    print 'Parsing', url, '...'

    for line in urllib2.urlopen (url):
        line = line.decode ('iso-8859-1').strip ()

        if not len (line):
            if curtag is not None:
                _autolearn_bibcode_tag (info, curtag, curtext)
                curtag = curtext = None
            continue

        if curtag is None:
            if line[0] == '%':
                # starting a new tag
                curtag = line[1]
                curtext = line[3:]
            elif line.startswith ('Retrieved '):
                if not line.endswith ('selected: 1.'):
                    die ('matched more than one publication')
        else:
            if line[0] == '%':
                # starting a new tag, while we had one going before.
                # finish up the previous
                _autolearn_bibcode_tag (info, curtag, curtext)
                curtag = line[1]
                curtext = line[3:]
            else:
                curtext += ' ' + line

    if curtag is not None:
        _autolearn_bibcode_tag (info, curtag, curtext)

    return info


def _extract_unixref_name (personelem):
    # XXX: deal with "The Fermi-LAT Collaboration", "Gopal-Krishna", etc.

    given = personelem.find ('given_name').text
    sur = personelem.find ('surname').text
    return given + ' ' + sur.replace (' ', '_')


def autolearn_doi (doi):
    import xml.etree.ElementTree as ET

    url = ('http://crossref.org/openurl/?id=%s&noredirect=true&pid=%s&'
           'format=unixref' % (urllib2.quote (doi), urllib2.quote (crossref_api_key)))
    info = {'doi': doi, 'keep': 0} # because we're autolearning

    # XXX sad to be not doing this incrementally, but Py 2.x doesn't
    # seem to have an incremental parser built in.

    print 'Parsing', url, '...'
    xmldoc = ''.join (urllib2.urlopen (url))
    root = ET.fromstring (xmldoc)

    jelem = root.find ('doi_record/crossref/journal')
    if jelem is None:
        die ('no <journal> element as expected in UnixRef XML for %s', doi)

    try:
        info['authors'] = [_extract_unixref_name (p) for p in
                           jelem.findall ('journal_article/contributors/person_name')]
    except:
        pass

    try:
        info['firstsurname'] = jelem.find ('journal_article/contributors/'
                                           'person_name[0]/surname').text
    except:
        pass

    try:
        info['title'] = ' '.join (t.strip () for t in
                                  jelem.find ('journal_article/titles/title').itertext ())
    except:
        pass

    try:
        info['year'] = int (jelem.find ('journal_issue/publication_date/year').text)
    except:
        pass

    return info


_atom_ns = '{http://www.w3.org/2005/Atom}'
_arxiv_ns = '{http://arxiv.org/schemas/atom}'

def _extract_arxiv_name (auth):
    # XXX I assume that we don't get standardized names out of Arxiv, so
    # nontrivial last names will be gotten wrong. I don't see any point
    # in trying to solve this here.
    return auth.find (_atom_ns + 'name').text


def autolearn_arxiv (arxiv):
    import xml.etree.ElementTree as ET

    url = 'http://export.arxiv.org/api/query?id_list=' + urllib2.quote (arxiv)
    info = {'arxiv': arxiv, 'keep': 0} # because we're autolearning

    # XXX sad to be not doing this incrementally, but Py 2.x doesn't
    # seem to have an incremental parser built in.

    print 'Parsing', url, '...'
    xmldoc = ''.join (urllib2.urlopen (url))
    root = ET.fromstring (xmldoc)
    ent = root.find (_atom_ns + 'entry')

    try:
        info['abstract'] = ent.find (_atom_ns + 'summary').text
    except:
        pass

    try:
        info['authors'] = [_extract_arxiv_name (a) for a in
                           ent.findall (_atom_ns + 'author')]
    except:
        pass

    info['firstsurname'] = info['authors'][0].rsplit (' ', 1)[1]

    try:
        info['doi'] = ent.find (_arxiv_ns + 'doi').text
    except:
        pass

    try:
        info['title'] = ent.find (_atom_ns + 'title').text
    except:
        pass

    try:
        info['year'] = int (ent.find (_atom_ns + 'published').text[:4])
    except:
        pass

    if 'doi' in info:
        info['bibcode'] = doi_to_maybe_bibcode (info['doi'])

    return info


# Commands

def cmd_apage (argv):
    if len (argv) != 2:
        raise UsageError ('expected exactly 1 argument')

    import webbrowser
    from urllib2 import quote

    idtext = argv[1]

    with connect () as db:
        try:
            id = db.locate_pub (idtext)
        except PubLocateError as e:
            die (e)

        arxiv = list (db.execute ('SELECT arxiv FROM pubs WHERE id = ?', (id, )))[0][0]
        if arxiv is None:
            die ('cannot open arxiv website: no identifier for record')

        url = 'http://arxiv.org/abs/' + quote (arxiv)
        webbrowser.open (url)


def cmd_init (argv):
    mkdir_p (bibpath ())

    if os.path.exists (dbpath):
        die ('the file "%s" already exists', dbpath)

    with connect () as db:
        try:
            init = open (datapath ('schema.sql')).read ()
            db.executescript (init)
        except sqlite3.OperationalError as e:
            die ('cannot initialize "%s": %s', dbpath, e)


def cmd_info (argv):
    if len (argv) != 2:
        raise UsageError ('expected exactly 1 argument')

    idtext = argv[1]

    with connect () as db:
        # TODO: should be OK to match multiple publications and print them all
        # out.

        try:
            id = db.locate_pub (idtext, autolearn=True)
        except PubLocateError as e:
            die (e)

        for pub in db.pub_query ('id = ?', id):
            year = pub.year or 'no year'
            title = pub.title or '(no title)'

            authors = list (db.get_pub_authors (pub.id))
            if len (authors):
                authstr = ', '.join (a[1] for a in authors)
            else:
                authstr = '(no authors)'

            print title
            print authstr, '(%s)' % year

            if pub.arxiv is not None:
                print 'arxiv:', pub.arxiv
            if pub.bibcode is not None:
                print 'bibcode:', pub.bibcode
            if pub.doi is not None:
                print 'DOI:', pub.doi

            if pub.abstract is not None:
                print
                # XXX linewrapping etc
                print pub.abstract


_bibtex_replacements = (
    '\\&ap;', u'~',
    '\\&#177;', u'±',
    '\&gt;~', u'⪞',
    '\&lt;~', u'⪝',
    '{', u'',
    '}', u'',
    '<SUP>', u'^',
    '</SUP>', u'',
    '<SUB>', u'_',
    '</SUB>', u'',
    'Delta', u'Δ',
    'Omega', u'Ω',
    '( ', u'(',
    ' )', u')',
    '[ ', u'[',
    ' ]', u']',
    ' ,', u',',
    ' .', u'.',
    ' ;', u';',
    '\t', u' ',
    '  ', u' ',
)

def _fix_bibtex (text):
    """Ugggghhh. So many problems."""

    if text is None:
        return None

    text = unicode (text)

    for i in xrange (0, len (_bibtex_replacements), 2):
        text = text.replace (_bibtex_replacements[i], _bibtex_replacements[i+1])
    return text


def sniff_url (url):
    p = 'http://dx.doi.org/'
    if url.startswith (p):
        return 'doi', urllib2.unquote (url[len (p):])

    p = 'http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode='
    if url.startswith (p):
        return 'bibcode', urllib2.unquote (url[len (p):])

    p = 'http://labs.adsabs.harvard.edu/ui/abs/'
    if url.startswith (p):
        return 'bibcode', urllib2.unquote (url[len (p):])

    p = 'http://arxiv.org/abs/'
    if url.startswith (p):
        return 'arxiv', urllib2.unquote (url[len (p):])

    p = 'http://arxiv.org/pdf/'
    if url.startswith (p):
        return 'arxiv', urllib2.unquote (url[len (p):])

    return None, None


def parse_bibtex_author (name):
    # generically: von Foo, Jr, Bob C. ; citeulike always gives us comma form
    a = [i.strip () for i in name.split (',')]

    if len (a) == 0:
        warn ('all last name, I think: %s', name)
        return a.replace (' ', '_')

    first = a[-1]
    surname = (',_'.join (a[:-1])).replace (' ', '_')

    if not len (first):
        warn ('CiteULike mis-parsed name, I think: %s', name)

    return first + ' ' + surname


def _ingest_one (db, rec, knownauthors):
    abstract = rec.get ('abstract')
    arxiv = rec.get ('eprint')
    bibcode = rec.get ('bibcode')
    doi = rec.get ('doi')
    nickname = rec.get ('id')
    title = rec.get ('title')
    year = rec.get ('year')

    if year is not None:
        year = int (year)

    if 'author' in rec:
        authors = [parse_bibtex_author (_fix_bibtex (a)) for a in rec['author']]
        firstsurname = authors[0].rsplit (' ', 1)[1].replace ('_', ' ')
    else:
        authors = None
        firstsurname = None

    abstract = _fix_bibtex (abstract)
    title = _fix_bibtex (title)

    # Augment information with what we can get from URLs

    urlinfo = []

    if 'url' in rec:
        urlinfo.append (sniff_url (rec['url']))

    for k, v in rec.iteritems ():
        if k.startswith ('citeulike-linkout-'):
            urlinfo.append (sniff_url (v))

    for kind, info in urlinfo:
        if kind is None:
            continue

        if kind == 'bibcode' and bibcode is None:
            bibcode = info

        if kind == 'doi' and doi is None:
            doi = info

        if kind == 'arxiv' and arxiv is None:
            arxiv = info

    # Shape up missing bibcodes

    if bibcode is None and doi is not None:
        bibcode = doi_to_maybe_bibcode (doi)
        print 'mapped', doi, 'to', bibcode or '(lookup failed)'

    # Ready to insert.

    info = dict (abstract=abstract, arxiv=arxiv, authors=authors,
                 bibcode=bibcode, doi=doi, nickname=nickname, title=title,
                 year=year)
    db.learn_pub (info)


def cmd_ingest (argv):
    if len (argv) != 2:
        raise UsageError ('expected exactly 1 argument')

    bibpath = argv[1]
    from bibtexparser.bparser import BibTexParser
    from bibtexparser.customization import author, type, convert_to_unicode

    custom = lambda r: author (type (convert_to_unicode (r)))

    with open (bibpath) as bibfile, connect () as db:
        knownauthors = dict (db.execute ('SELECT name, oid FROM author_names'))
        bp = BibTexParser (bibfile, customization=custom)

        for rec in bp.get_entry_list ():
            _ingest_one (db, rec, knownauthors)


def cmd_jpage (argv):
    if len (argv) != 2:
        raise UsageError ('expected exactly 1 argument')

    import webbrowser
    from urllib2 import quote

    idtext = argv[1]

    with connect () as db:
        try:
            id = db.locate_pub (idtext)
        except PubLocateError as e:
            die (e)

        doi = list (db.execute ('SELECT doi FROM pubs WHERE id = ?', (id, )))[0][0]
        if doi is None:
            die ('cannot open journal website: no DOI for record')

        url = 'http://dx.doi.org/' + quote (doi)
        webbrowser.open (url)


def cmd_read (argv):
    if len (argv) != 2:
        raise UsageError ('expected exactly 1 argument')

    idtext = argv[1]

    with connect () as db:
        try:
            id = db.locate_pub (idtext, autolearn=True)
        except PubLocateError as e:
            die (e)

        res = list (db.execute ('SELECT sha1 FROM pdfs WHERE pubid = ?', (id, )))
        if len (res):
            sha1 = res[0][0]
        else:
            proxy = setup_proxy ()
            sha1 = db.try_get_pdf_for_id (proxy, id)

    if sha1 is None:
        die ('no saved PDF for %s, and cannot figure out how to download it', idtext)

    # XXX may want to centralize this; also, close FD's, etc.

    try:
        pid = os.fork ()
    except OSError as e:
        die ('cannot fork() first time: %s', e)

    if pid == 0:
        os.setsid () # become new session leader, apparently a good thing to do.

        try:
            pid2 = os.fork ()
        except OSError as e:
            die ('cannot fork() second time: %s', e)

        if pid == 0:
            os.execlp (pdfreader, pdfreader, libpath (sha1, 'pdf'))
        else:
            os._exit (0)
    else:
        pass # parent has nothing to do


def cmd_setpdf (argv):
    if len (argv) != 3:
        raise UsageError ('expected exactly 2 arguments')

    idtext = argv[1]
    pdfpath = argv[2]

    import hashlib, shutil

    with connect () as db:
        # Check that we know what pub we're talking about
        try:
            id = db.locate_pub (idtext)
        except PubLocateError as e:
            die (e)

        # Get SHA1 of the PDF
        with open (pdfpath) as f:
            s = hashlib.sha1 ()

            while True:
                b = f.read (4096)
                if not len (b):
                    break

                s.update (b)

            sha1 = s.hexdigest ()

        # Copy it into the library
        ensure_libpath_exists (sha1)
        dest = libpath (sha1, 'pdf')
        shutil.copyfile (pdfpath, dest)

        # Update the DB
        db.execute ('INSERT OR REPLACE INTO pdfs VALUES (?, ?)', (sha1, id))


def cmd_setsecret (argv):
    if len (argv) != 1:
        raise UsageError ('expected no arguments')

    if not sys.stdin.isatty ():
        die ('this command can only be run with standard input set to a TTY')

    store_user_secret ()


# Toplevel driver infrastructure

def usage ():
    print 'usage goes here'


def driver (argv):
    if len (argv) == 1 or argv[1] == '--help':
        usage ()
        return

    cmdname = argv[1]
    cmdfunc = globals ().get ('cmd_' + cmdname.replace ('-', '_'))

    if not callable (cmdfunc):
        die ('"%s" is not a recognized subcommand; run me without '
             'arguments for usage help', cmdname)

    try:
        cmdfunc (argv[1:])
    except UsageError as ue:
        # TODO: synopsize command-specific usage help as an attribute on the
        # function (so we can auto-gen a multi-command usage summary too)
        raise SystemExit ('usage error: ' + ue.bibmsg)

if __name__ == '__main__':
    driver (sys.argv)
